\section{Motivations}

Web attacks are nowadays considered the most important source of loss of financial and intellectual property.
Thanks to the high availability of an Internet connection in the modern world, these kind of attacks are getting more and more common, targeting not only institutions and high-profile companies, but also medium-small size companies owning a web server and common users, causing overall a massive stealing of valuable personal user information and financial losses in the order of millions of Euro.

Moreover, the increase of different vehicles for browsing the web, like tablets and smartphones, makes web-related attacks a very appealing target for criminals and at the same time a more difficult challenge for securing all possible holes which could allow a potential attacker to take control of the system.

This trend is also reflected in the topic of academic research. In fact, it can be shown as in the last few years a large number of papers, seminars and workshops published in the most important conferences all over the world cover web-related attacks and defences.

It is possible to categorize these studies in three main sectors:
\begin{itemize}
\item analysis of vulnerabilities related to web applications, web servers, or web browsers, and on the way these components get compromised;
\item dissection and analysis of the internals of specific attack campaigns;
\item proposal of new protection mechanisms to mitigate existing attacks.
\end{itemize}
The result is that almost all the web infections panorama has been studied in detail: how attackers scan the web or use Google dorks to find vulnerable applications, how they run automated attacks, and how they deliver malicious content to the final users.

There is still a missing piece in this scenario: there are only a few academic works which sufficiently detail the behaviour of an average attacker during and after a website is compromised.
Even if there are cases where the attacker is only interested in some informations stored in the service itself, like the dump of a database following an SQL injection, in the majority of the cases the attacker wants to maintain access to the compromised machine and include it as part of a larger malicious infrastructure (e.g., to act as part of a botnet or to deliver malicious documents to the users who visit the page).

While the recent literature often focuses on catchy topics and future trends, such as drive-by downloads and black-hat SEO, this is just the tip of the iceberg. In fact, there is a wide variety of malicious activities performed on the Internet on a daily basis, with goals that are often different from those of the high-profile cyber criminals who attract the media and the security firms' attention.

The main reason for the lack of previous works in this direction of research is that almost all existing projects (further details will be provided in the next section) of web honeypots use fake applications. With this kind of approach no real attacks can be actually preformed and all steps commonly performed by the attacker after the vulnerability exploitation are missing.

Other than academic works, security companies often relied on informations provided by clients in order to better understand the motivation of the various classes of attackers. For example, in a recent survey conducted by Commtouch and the StopBadware organization \cite{stopbadawareSurvey}, 600 owners of compromised websites have been asked to fill a questionnaire to report what the attacker did after exploiting the website. This is obviously a very naive approach, as different owners have different understanding of how the attack affected their website, with the consequence of having partial or completely wrong informations. Furthermore, a survey is difficult to serialize and automatize in order to extract valuable informations for research purposes.

This work points to fulfil this hole: first we present a fully functional honeypot network, where applications are real and completely exploitable by attackers. Then we analyse the behaviour of the attackers, analysing the files uploaded and the most common methods and techniques used. Finally we try to understand the reasons and probable goals behind such attacks.

The web application deployed are specifically tailored in order to be attractive for the attacker interested in gaining and maintaining control over the machine after the exploitation, privileging specific vulnerabilities aimed to allow this kind of behaviour, such remote file upload and administrator password reset, rather than other common vulnerabilities more focused on information gathering, like SQL injection, or user's credentials stealing, like XSS.

\section{Related Works}

A honeypot is one of the most common automated system available for security researcher to investigate real world trend and phenomena on widespread networks.

The concept of a honeypot on a network first began in 1999 when Lance Spitzner, founder of the Honeynet Project \cite{honeynetProject}, published the paper ``To Build a Honeypot'', where he defines a honeypot as:
\begin{quote}
A honeypot is a high interaction fake agent that simulates a production network and configured such that all activity is monitored, recorded and, in a degree, discreetly regulated.
\end{quote}
Speaking about Internet network, we can basically classify honeypots in two different categories:
\begin{description}
\item[Client Honeypots,] which look for malicious servers and detect exploits by actively connecting to servers, downloading and executing files;
\item[Server Honeypots,] which attract attackers by exposing one or more known vulnerable services.
\end{description}
Our research focused on the second category, as we wanted to investigate attackers behaviours after a web application has been compromised.

We can divide server honeypots in two subcategories, according to their behaviour toward the users of the service:
\begin{description}
\item[Low-Interaction] honeypots: as can be guessed by its name, this kind of honeypots only simulates the application without actually deploying any real service (and therefore can only observe attacks and can't really be exploited). These honeypots usually have limited capabilities but can be useful for detecting network probes or automated attack services (e.g., malicious search engines or common dorks). Examples of these are Honeyd \cite{honeyd}, Leurre.com \cite{leurre} and SGNET \cite{sgnet}, which are able to emulate several operating systems and services;
\item[High-Interaction] honeypots: this class of honeypots offers a fully functional environment that can be completely compromised by the attacker. Attacker's behaviour, commands executed and uploaded files can be fully tracked, offering a more useful insight into its modus operandi, but with higher maintenance costs. These honeypots are usually deployed as virtual machine, allowing for a fast restore of the original state after the system has been compromised. An example of this honeypot system on ssh service can be found here \cite{highhoney}.
\end{description}

The study of attacks against web applications is often done through the deployment of web honeypots. Several different approaches have been considered in the deployment of low-interaction honeypot networks, such as Glastopf \cite{glastopf} and the DShield Web Honeypot Project \cite{dswhp}, based on the idea of using templates and known patterns in order to mimic several vulnerable web application, or the Google Hack honeypot \cite{googleHoney}, designed to attract attackers who use search engines to find vulnerable web applications. Another interesting approach has been proposed by John et al. \cite{johnhsh} using search engines logs in order to identify malicious queries (queries aimed to list vulnerable web applications, commonly known as ``dorks'') and to automatically generate and deploy honeypot pages responding to the observed search criteria. This latter study in particular showed some interesting results: the authors found out that the median time for honeypot pages to be attacked after they have been crawled by a search engine spider is 12 days, and that local file disclosure vulnerabilities seem to be the most sought after by attackers, accounting to more than 40\% of the malicious requests received by their honeypots. Other very common attack patterns were trying to access specific files (e.g.web application installation scripts), and looking for remote file inclusion vulnerabilities. A common characteristic of all these patterns is that they are very suitable for an automatic attack, as they only require to access some fixed paths or trying to inject precomputed data in URL query strings.

The main weakness in all these systems is their inability to disguise themselves as real websites to manual attackers. Low interaction honeypots, in fact, can collect data from crawlers and automated script, but human attackers can quickly realize that the system is a trap and not a real functional application (no images present, no text present etc.).

The only real attempt for realizing high-interaction web honeypots has been done by HIHAT \cite{hihat} toolkit. However, the results from this experiment did not contain any interesting finding, as the machines run for few days only and the honeypot received only 8000 hits, mostly from benign crawlers.

Nevertheless, some work on categorizing the attackers' behaviour has been done on interactive shells of high-interaction honeypots running SSH, like in the work from V. Nicomette et al\cite{highhoney}. Another study, performed by D. Ramsbrock et al. \cite{sshprofiling}, used the very same system in order to perform a first profiling of attackers. Their results showed how attackers seem to separate phases assigning different tasks to different machines (i.e. scans and SSH bruteforce attacks are run from machines that are different from the ones used for intrusion), and that most of the attacks follow a rigid list of passages, using very similar attack methods and sequences of commands, suggesting that most attackers are actually following cookbooks that can be found on the Internet. Looking at the commands issued on these SSH honeypots, this study shows how the main activities performed on the systems were checking the software configuration, and trying to install malicious software, such as a botnet scripts or a backdoor, in order to keep in contact with the victim machine after the compromisation.

Finally, our work does not only concern the profiling of attackers' behaviour, but also the categorization of files uploaded to our honeypots. Several studies have been proposed regarding the automatic detection of similarities between source code files and binaries, especially for plagiarism detection, as in the work of Chen et al. \cite{plagdet1} or in the work of Saebjornsen et al. \cite{plagdet2} regarding binary executables. Over the years several other frameworks have been published for different formats (especially images and other multimedia formats), mostly with the same purpose.

The difference between the datasets used in these studies and in our case study is that we have much more variety in files. Our collection includes examples of multimedia, image, source code and binary files. Furthermore, many of the source code files use some degree of obfuscation, making plagiarism detection methods useless. On top of that we needed a method able to compare large datasets of files in a very quick way, as the clustering should be done daily against the whole past collection, while plagiarism detection is very resource and time demanding.

The problem of classifying and fingerprinting files of any type in a decent amount of time and using as less computing power as possible has, however, been studied carefully in the area of forensics. In particular, some tools based on the idea of similarity digest have been published in the last few years, like Ssdeep \cite{ssdeep} and Sdhash \cite{sdhash}. These approaches have been proven to be reliable and fast with regard to the detection of similarities between files of any kind, being based on the byte-stream representation of data. We chose to follow this approach, and use the two tools mentioned before for our work.
